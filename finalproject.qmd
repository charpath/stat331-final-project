---
title: "STAT 331 Final Project"
subtitle: "GitHub: https://github.com/charpath/stat331-final-project"
author: "Charles Jansen, Stanley Lam, Minh Ho-Hoang, James Lamkin"
format: 
  html:
    html-table-processing: none
    theme: default
    code-tools: true
    toc: true
    embed-resources: true
    code-fold: true
execute: 
  echo: true
  eval: true
  error: true
  message: false
  warning: false
---

## Part 1: Project Proposal + Data

### Data Description

```{r setup}
# Load necessary libraries here
library(tidyverse)
library(knitr)
library(kableExtra)
library(gganimate)
library(gifski)
```

```{r}
# Load data here & set seed for simulation reproducibility
murders <- read_csv("murder_per_100000_people.csv")
unemployment <- read_csv("long_term_unemployment_rate_percent.csv")
set.seed(331)
```

The data used in this analysis originates from the Gapminder Foundation, an independent educational non-profit organization dedicated to fighting global misconceptions. We have specifically utilized two datasets from their collection: "Murders per 100,000 People" and "Long-Term Unemployment Rate". These datasets provide insights into societal trends across various countries over time.

##### "Murders per 100,000 People" Dataset 
This dataset contains the annual number of murders per 100,000 people for various countries, spanning from 1950 to 2016. It offers a measure of violent crime rates, providing a crucial indicator of societal safety and stability.

##### "Long-Term Unemployment Rate (Percentage)" Dataset
This dataset presents the annual long-term unemployment rate as a percentage of the labor force for various countries, covering the period from 1990 to 2020. Long-term unemployment is defined as individuals who have been unemployed for a year or longer. This metric serves as an indicator of economic health and labor market stability.

These datasets were chosen to explore the potential relationship between long-term unemployment rates and murder rates. The hypothesis is that economic factors, such as long-term unemployment, may correlate with societal indicators of violence â€” in this case murder. So, we hypothesize that an increase in unemployment rates will likely also lead to an increase in murder rates.


### Cleaning Murders Data

First, we'll start with the murders dataset. 

The types of all of the columns seem correct: The country is represented as a character vector, and all of the actual murder rates are numerical. It doesn't seem like we'll need any particular cleaning, so we'll go ahead and pivot our data.

We want to pivot every column that isn't the country into two columns: The year, and the murders per 100,000 in that country and that year.

Additionally, we can convert the year to a number, and drop any rows that do not have an observation for the murders per 100,000.

```{r}
murders_clean <- murders |> 
  pivot_longer(cols = !country, names_to = "year", values_to = "murders_per_100000") |> 
  mutate(year = as.numeric(year)) |> 
  filter(!is.na(murders_per_100000))
```

### Cleaning Unemployment Data

Now that the murders data is clean and pivoted, what about the unemployment data?

Similarly to the murders data, it looks good from the start. We'll do the exact same thing to this data as we did to the murders data: Pivot all of the observation columns into one year column and one unemployment rate column, then convert year to a number and drop all missing observations.

```{r}
unemployment_clean <- unemployment |> 
  pivot_longer(cols = !country, names_to = "year", values_to = "long_term_unemployment_rate") |> 
  mutate(year = as.numeric(year)) |> 
  filter(!is.na(long_term_unemployment_rate))
```

### Joining Datasets

Before we join the two datasets, I'd like to make a note:

```{r}
data.frame("Unemployment" = range(unemployment_clean$year),
           "Murders" = range(murders_clean$year),
           row.names = c("Earliest Observation", "Latest Observation")) |> 
  kable() |> 
  kable_styling(full_width = F)
```

As we can see from this table, the unemployment data only goes from 1990 to 2020, while the murders data spans all the way from 1950 to 2016. This means, after joining the two datasets, we will only be able to work with data from 1990 to 2016, or about 26 years worth of data. This shouldn't have a huge impact on anything, but it's good to keep in mind.

Now that that's out of the way, we can join the two datasets into one variable. One thing to keep in mind is that we'll only be keeping year-country combinations that have an observation for both unemployment rate and murders per 100,000.

```{r}
murders_unemployment <-
  inner_join(murders_clean, unemployment_clean,
             by = join_by(country == country, year == year))
```

There it is! We have our two datasets fully cleaned and joined into one table that we can continue to work with.

## Part 2: Linear Regression

With the clean data, we will now compare the relationship between our two quantitative variables using a scatterplot. Unemployment will be our explanatory variable, while murders will be our response variable.

### Visual Plots

```{r}
murders_unemployment |>
  ggplot(aes(x = long_term_unemployment_rate, y = murders_per_100000)) +
  geom_point() +
  geom_smooth(method = "lm") +
  labs(title = "Relationship Between Murder and Unemployment Rates in Different Countries
(1990 - 2016)",
       x = "Unemployment Rate (%)",
       subtitle = "Murders (per 100,000)") +
  theme(axis.title.y = element_blank(),
        axis.title.x = element_text(size = 12),
        axis.text.x = element_text(size = 10),
        axis.text.y = element_text(size = 10))
```

When looking at the linear regression, it appears that there is a weak negative linear relationship between unemployment rate and murder rate. There is also multiple outliers, on the top of the scatterplot where murder rates are rampant. Now we will compare these values over time to see if anything has changed over the years.

```{r}
anim <- murders_unemployment |>
  ggplot(aes(x = long_term_unemployment_rate, y = murders_per_100000)) +
  geom_point(size = 3) + 
  labs(title = "Year: {round(frame_time)}",
       x = "Unemployment Rate (%)", y = "Murder (per 100,000)") +
  transition_time(year) +
  ease_aes("linear")

animate(anim, renderer = gifski_renderer())
```

From the animated visual, there is not much we can describe about the relationship between unemployment rate and murder rate. However, we are able to depict murder rate and unemployment rate seperately. In the late 1990s, we see murder rate spike up and gradually decrease as we enter the new millennium. And as the 2008 recession starts to hit, we see unemployment rate spike up for a couple of years.  

### Linear Regression

Now that we've been able to take a look at a visual, let's see if we can fit a linear regression model to predict the murders per 100,000 based on the unemployment rate. Just like before, murders per 100,000 will be our response variable, and unemployment rate will be our explanatory variable.

First, in order to make fitting the model slightly easier, we're going to condense each country's observations down into one row by taking the median value of the observations and taking the natural log of our observations.  

```{r}
lr_murders_unemployment <- murders_unemployment |> 
  group_by(country) |> 
  summarize(med_murders_per_100000 = median(murders_per_100000),
            med_unemployment = median(long_term_unemployment_rate)) |>
  mutate(ln_med_murders_per_100000 = log(med_murders_per_100000))
```

Now that we have our condensed data, we should plot it before we actually fit the model. The following plot shows our condensed data, with each point representing one country's median murder and unemployment rates across all of the years in the data. 

```{r}
lr_murders_unemployment |> 
  ggplot(mapping = aes(x = med_unemployment, y = ln_med_murders_per_100000)) +
  geom_point() +
  geom_smooth(method = "lm") +
  labs(title = "Relationship Between Median Murder and Unemployment Rates in Different Countries",
       x = "Unemployment Rate (%)",
       subtitle = "ln(Murders (per 100,000))") +
  theme(axis.title.y = element_blank(),
        axis.title.x = element_text(size = 12),
        axis.text.x = element_text(size = 10),
        axis.text.y = element_text(size = 10))
```

As we can see from this plot, there doesn't seem to be very much of a relationship between the two variables. 

```{r}
mu_model <- lm(ln_med_murders_per_100000~med_unemployment, data = lr_murders_unemployment)
broom::tidy(mu_model) |> 
  kable()
```
Looking at broom's tidy output, we can see that the prediction equation of our model is as follows:

$$\widehat{\ln(Median\ Murders\ per\ 100,000)}\ =\ 0.339\ -\ 0.0293(Median\ Unemployment)$$

The intercept is 0.339, meaning when unemployment rate is at 0%, we expect to have $e^{0.339}$ or 1.4 murders per 100,000. The coefficient of Median Unemployment Rate is 0.0293, so for each 1% unemployment rate increases by in a country, we expect the murders per 100,000 to increase by a factor of $e^{0.0293}$.

### Model Fit

Now let's look at how well a linear model fits our data. We can find out by calculating the variation in our response variable (murders per 100,000), the fitted model values and residuals. 

```{r}
observation_col <- select(lr_murders_unemployment, med_murders_per_100000)
fitted_resid <- broom::augment(mu_model) |>
  select(.fitted, .resid) |>
  bind_cols(observation_col) |>
  summarize(across(everything(), var, .names = "variance_{col}")) |>
  pivot_longer(cols = everything(),
               names_to = "Variance_Type",
               values_to = "Variance") |>
  mutate(Variance_Type = case_when(Variance_Type == "variance_.fitted" ~ "Fitted Values",
                                   Variance_Type == "variance_.resid" ~ "Residuals",
                                   Variance_Type == "variance_med_murders_per_100000" ~ "Response Variable"))

fitted_resid |>
  kable(col.names = c("Variance Type", "Variance"),
        caption = "Variation of Different Values",
        digits = c(0, 3)) |>
  kable_classic(full_width = F,
                bootstrap_options = "striped") |> 
  row_spec(row = 0, bold = T, align = "c")

```

If the variance of the response variable represents the total amount of variability, then the variance of the fitted values represent the proportion of the total variablility that is explained by our linear model, and the variance of the residuals represent the proportion of the total variablility that is not explained by our linear model. In our case, our linear model explained 0.435% of the total variability, making our linear model useless to observe the relationship between unemployment and murder. Our scatterplot also backs this up. Although our line of best fit seems to fit in most of the data, we have many "outliers" data that seems to not follow the pattern, suggesting that a linear model may not be the best model in representing our data even after a transformation and that we might not even be able to fit a linear model because there is no relationship between the two variables.

## Simulation

Regardless, for the sake of the report, we will try to simulate what our outcomes should look like if it were a linear relationship. 

### Visualizing Simulations

First, we will simulate sampling the data as if the data was linear with residuals that follow a normal distribution. Then we'll extract $R^2$ to find out the strength of the relationship between our simulated data and our observed data. 

```{r}
pred_murder <- predict(mu_model)

est_sigma <- sigma(mu_model)

rand_error <- function(x, mean = 0, sd){
  
  error <- rnorm(length(x), mean, sd)
  
  return(x + error)
}

sim_response <- tibble(sim_murder = rand_error(pred_murder, sd = est_sigma))

simulation <- lr_murders_unemployment |>
  bind_cols(sim_response)

mod2 <- lm(sim_murder ~ med_murders_per_100000, data = simulation)
r_squared <- summary(mod2)$r.squared
r_squared
```

The $R^2$ is `r round(r_squared, 4)` meaning that virtually none of the variability in our observed data is explained by or simulation results. Our simulated data is not a good explanation of our observed data. 

```{r}
simulation |>
  ggplot(aes(x = sim_murder, y = med_murders_per_100000)) +
  geom_point() +
  geom_smooth(method = "lm") +
  labs(x = "Simulated Murders per 100000", y = "Observed Murders per 100000",
       title = "Simulated Data VS Observed")
```

This plot showing the relationship between simulating murders and observed murders backs up the extremely low $R^2$ value. If the simulation was a good explainer for our observations, we'd expect to see a strong positive relationship between our simulation and observation. This plot tells us that our observations are nowhere near the expected murder rate. 

```{r}
simulation |>
  ggplot(aes(x = med_unemployment, y = med_murders_per_100000)) +
  geom_point() +
  labs(title = "Observed Data Across Countries")

simulation |>
  ggplot(aes(x = med_unemployment, y = sim_murder)) +
  geom_point() +
  labs(title = "Simulated Data Across Countries")
```

The relationship between the observed murder rate and simulated murder rate to unemployment looks quite different hence the low $R^2$ value. Our simulated data looks more scattered than our observed data however, both data was clustered to the left since we don't have many countries with consistantly high unemployment.

### Generating Multiple Predictive Checks

We only looked at one simulation and discussed one $R^2$ value. That one $R^2$ value only told us if that one perticular simulation was a good model to explain our observations. We don't know if that simulation was a typical simulation or we got really unlucky and generated a really unusual simulation. So, we need to generate many more simulations to compare to our observed data to create a distribution of $R^2$ values. This will allow us to find out what is an average $R^2$ value and based on that value, how well does simulated data explain our observations on avgerage.

```{r}
num_simulations <- 1000
simulated_r_squared <- numeric(num_simulations)

for(i in 1:num_simulations){
  sim_response <- tibble(sim_murder = rand_error(pred_murder, sd = est_sigma))
  
  simulation <- lr_murders_unemployment %>%
    bind_cols(sim_response)
  
  complete_data <- simulation %>% 
    filter(complete.cases(.)) 
  
  sim_obs_model <- lm(med_murders_per_100000 ~ sim_murder, data = complete_data)
  simulated_r_squared[i] <- summary(sim_obs_model)$r.squared
}

sim_r_sq_df <- data.frame(r_squared = simulated_r_squared) 

original_r_squared <- summary(mu_model)$r.squared

ggplot(sim_r_sq_df, aes(x = r_squared)) +
  geom_histogram(bins = 20) +
  geom_vline(aes(xintercept = original_r_squared), color = 'red') +
  labs(title = "Distribution of R-squared Values", caption = "*Red line represents original R-squared value", x = "R-squared", y = "Frequnecy") +
  theme_minimal()
```
This is 1,000 $R^2$ values ploted on a plot. It is skewed to the left with our first $R^2$ value being close to the skew meaning that we were not unlucky with our first simulation and that typically, simulation does a poor job at explaining our observations. Our observations does not follow a linear model even after a transformation meaning our two variables are likely not related to one another. 