---
title: "STAT 331 Final Project"
subtitle: "GitHub: https://github.com/charpath/stat331-final-project"
author: "Charles Jansen, Stanley Lam, Minh Ho-Hoang, James Lamkin"
format: 
  html:
    html-table-processing: none
    theme: default
    code-tools: true
    toc: true
    embed-resources: true
    code-fold: true
execute: 
  echo: true
  eval: true
  error: true
  message: false
  warning: false
---

## Part 1: Project Proposal + Data

### Data Description

```{r setup}
# Load necessary libraries here
library(tidyverse)
library(knitr)
library(kableExtra)
library(gganimate)
library(gifski)
```

```{r}
# Load data here
murders <- read_csv("murder_per_100000_people.csv")
unemployment <- read_csv("long_term_unemployment_rate_percent.csv")
```

The data used in this analysis originates from the Gapminder Foundation, an independent educational non-profit organization dedicated to fighting global misconceptions. We have specifically utilized two datasets from their collection: "Murder per 100,000 People" and "Long-Term Unemployment Rate". These datasets provide insights into societal trends across various countries over time.

##### "Murder per 100,000 People" Dataset 
This dataset contains the annual number of murders per 100,000 people for various countries, spanning from 1950 to 2016. It offers a measure of violent crime rates, providing a crucial indicator of societal safety and stability.

##### "Long-Term Unemployment Rate (Percentage)" Dataset
This dataset presents the annual long-term unemployment rate as a percentage of the labor force for various countries, covering the period from 1990 to 2020. Long-term unemployment is defined as individuals who have been unemployed for a year or longer. This metric serves as an indicator of economic health and labor market stability.

**VARIABLES ARE:**

**LONG TERM EMPLOYMENT RATE (%)** - Represents the percentage of the labor force that has been unemployed for a year or more.

**MURDERS PER 100,000 PEOPLE** - Represents the number of murders per 100,000 individuals in a given population.

These datasets were chosen to explore the potential relationship between long-term unemployment rates and murder rates. The hypothesis is that economic factors, such as long-term unemployment, may correlate with societal indicators of violence â€” in this case murder.


### Cleaning Murders Data

First, we'll start with the murders dataset. 

The types of all of the columns seem correct: The country is represented as a character vector, and all of the actual murder rates are numerical. It doesn't seem like we'll need any particular cleaning, so we'll go ahead and pivot our data.

We want to pivot every column that isn't the country into two columns: The year, and the murders per 100,000 in that country and that year.

Additionally, we can convert the year to a number, and drop any rows that do not have an observation for the murders per 100,000.

```{r}
murders_clean <- murders |> 
  pivot_longer(cols = !country, names_to = "year", values_to = "murders_per_100000") |> 
  mutate(year = as.numeric(year)) |> 
  filter(!is.na(murders_per_100000))

head(murders_clean) |> 
  kable(align = "ccc")
```

### Cleaning Unemployment Data

Now that the murders data is clean and pivoted, what about the unemployment data?

Similarly to the murders data, it looks good from the start. We'll do the exact same thing to this data as we did to the murders data: Pivot all of the observation columns into one year column and one unemployment rate column, then convert year to a number and drop all missing observations.

```{r}
unemployment_clean <- unemployment |> 
  pivot_longer(cols = !country, names_to = "year", values_to = "long_term_unemployment_rate") |> 
  mutate(year = as.numeric(year)) |> 
  filter(!is.na(long_term_unemployment_rate))

head(unemployment_clean) |> 
  kable(align = "ccc")
```

### Joining Datasets

Before we join the two datasets, I'd like to make a note:

```{r}
data.frame("Unemployment" = range(unemployment_clean$year),
           "Murders" = range(murders_clean$year),
           row.names = c("Earliest Observation", "Latest Observation")) |> 
  kable()
```

As we can see from this table, the unemployment data only goes from 1990 to 2020, while the murders data spans all the way from 1950 to 2016. This means, after joining the two datasets, we will only be able to work with data from 1990 to 2016, or about 26 years worth of data. This shouldn't have a huge impact on anything, but it's good to keep in mind.

Now that that's out of the way, we can join the two datasets into one variable. One thing to keep in mind is that we'll only be keeping year-country combinations that have an observation for both unemployment rate and murders per 100,000.

```{r}
murders_unemployment <-
  inner_join(murders_clean, unemployment_clean,
             by = join_by(country == country, year == year))

head(murders_unemployment) |> 
  kable(align = "cccc")
```

There it is! We have our two datasets fully cleaned and joined into one table that we can continue to work with.

=======

## Part 2: Linear Regression

With the clean data, we will now compare the relationship between our two quantitative variables using a scatterplot. Unemployment will be our explanatory variable, while murder will be our response variable.

### Visual Plots

```{r}
murders_unemployment |>
  ggplot(aes(x = long_term_unemployment_rate, y = murders_per_100000)) +
  geom_point() +
  geom_smooth(method = "lm") +
  labs(title = "Relationship Between Murder and Unemployment in Different Countries (1990 - 2016)",
       x = "Unemployment Rate (%)",
       subtitle = "Murder (1 per 100,000)") +
  theme(axis.title.y = element_blank(),
        axis.title.x = element_text(size = 12),
        axis.text.x = element_text(size = 10),
        axis.text.y = element_text(size = 10))
```

When looking at the linear regression, it appears that there is a weak negative linear relationship between unemployment rate and murder rate. There is also multiple outliers, on the top of the scatterplot where murder rates are rampant. Now we will compare these values over time to see if anything has changed over the years.

```{r}
anim <- murders_unemployment |>
  ggplot(aes(x = long_term_unemployment_rate, y = murders_per_100000)) +
  geom_point(size = 3) + 
  labs(title = "Year: {round(frame_time)}",
       x = "Unemployment Rate (%)", y = "Murder (per 100,000)") +
  transition_time(year) +
  ease_aes("linear")

animate(anim, renderer = gifski_renderer())
```

From the animated visual, there is not much we can describe about the relationship between unemployment rate and murder rate. However, we are able to depict murder rate and unemployment rate seperately. In the late 1990s, we see murder rate spike up and gradually decrease as we enter the new millennium. And as the 2008 recession starts to hit, we see unemployment rate spike up for a couple of years.  

### Linear Regression

Now that we've been able to take a look at a visual, let's see if we can fit a linear regression model to predict the murders per 100,000 based on the unemployment rate. Just like before, murders per 100,000 will be our response variable, and unemployment rate will be our explanatory variable.

First, in order to make fitting the model slightly easier, we're going to condense each country's observations down into one row by taking the median value of the observations. 

```{r}
lr_murders_unemployment <- murders_unemployment |> 
  group_by(country) |> 
  summarize(med_murders_per_100000 = median(murders_per_100000),
            med_unemployment = median(long_term_unemployment_rate))
```

Now that we have our condensed data, we can go ahead and fit the model:

```{r}
mu_model <- lm(med_murders_per_100000~med_unemployment, data = lr_murders_unemployment)
broom::tidy(mu_model) |> 
  kable()
```
Looking at broom's tidy output, we can see that the prediction equation of our model is as follows:

$\widehat{Median\ Murders\ per\ 100,000}\ =\ 2.941\ -\ 0.045(Median\ Unemployment)$

The intercept is 2.941, meaning when unemployment rate is at 0%, we expect to have 2.941 murders per 100,000. The coefficient of Median Unemployment Rate is -0.045, so for each 1% unemployment rate increases by in a country, we expect the murders per 100,000 to decrease by 0.045.

### Model Fit

Now lets look at how well a linear model fits our data. We can find out by calculating the variation in our response variable (murder per 100,000), the fitted model values and residuals. 

```{r}
mu_lm <- lm(murders_per_100000 ~ long_term_unemployment_rate, data = murders_unemployment)

#I got the values for the fitted and residuals, just need to find variance and put into table
#along with the varaince in our response variable
unempolyment_col <- select(murders_unemployment, murders_per_100000)
fitted_resid <- broom::augment(mu_lm) |>
  select(.fitted, .resid) |>
  bind_cols(unempolyment_col) |>
  summarize(across(everything(), var, .names = "variance_{col}")) |>
  pivot_longer(cols = everything(),
               names_to = "Variance_Type",
               values_to = "Variance") |>
  mutate(Variance_Type = case_when(Variance_Type == "variance_.fitted" ~ "Fitted Values",
                                   Variance_Type == "variance_.resid" ~ "Residuals",
                                   Variance_Type == "variance_murders_per_100000" ~ "Response Variable"))

fitted_resid |>
  kable(col.names = c("Variance Type", "Variance"),
        caption = "Variation of Different Values",
        digits = c(0, 3)) |>
  kable_classic(full_width = F,
                bootstrap_options = "striped") |> 
  row_spec(row = 0, bold = T, align = "c")

```

If the variance of the response variable represents the total amount of variability, then the variance of the fitted values represent the proportion of the total variablility that is explained by our linear model, and the variance of the residuals represent the proportion of the total variablility that is not explained by our linear model. In our case, our linear model explained 0.1% of the total variability, making our linear model useless to observe the relationship between unemployment and murder. Our scatterplot also backs this up. Although our line of best fit seems to fit in most of the data, we have many "outliers" data that seems to not follow the pattern, suggesting that a linear model may not be the best model in representing our data. 

## Simulation
```{r}
mod <- lm(murders_per_100000 ~ long_term_unemployment_rate, data = murders_unemployment)

pred_murder <- predict(mod)

est_sigma <- sigma(mod)

#function for error
rand_error <- function(x, mean = 0, sd){
  
  error <- rnorm(length(x), mean, sd)
  
  return(x + error)
}

sim_response <- tibble(sim_murder = rand_error(pred_murder, sd = est_sigma))

simulation <- murders_unemployment |>
  bind_cols(sim_response)

mod2 <- lm(sim_murder ~ murders_per_100000, data = simulation)
r_squared <- summary(mod2)$r.squared
r_squared
```

```{r}
simulation |>
  ggplot(aes(x = sim_murder, y = murders_per_100000)) +
  geom_point() +
  geom_smooth(method = "lm") +
  labs(x = "Simulated Murder per 100000", y = "Murder per 100000",
       title = "Our Simulated Data VS Observed")

simulation |>
  ggplot(aes(x = year, y = murders_per_100000)) +
  geom_point() +
  labs(title = "Observed Data Over Time (1990-2016")

simulation |>
  ggplot(aes(x = year, y = sim_murder)) +
  geom_point() +
  labs(title = "Simulated Data Over Time (1990-2016")
```

```{r}
num_simulations <- 1000
simulated_r_squared <- numeric(num_simulations)

for(i in 1:num_simulations){
  sim_response <- tibble(sim_murder = rand_error(pred_murder, sd = est_sigma))
  
  simulation <- murders_unemployment %>%
    bind_cols(sim_response)
  
  # Remove rows containing NAs
  complete_data <- simulation %>% 
    filter(complete.cases(.)) 
  
  mod2 <- lm(murders_per_100000 ~ sim_murder, data = complete_data)
  simulated_r_squared[i] <- summary(mod2)$r.squared
}

# Plot the distribution of r squared values
sim_r_sq_df <- data.frame(r_squared = simulated_r_squared) 

# Original r squared
original_r_squared <- summary(mod)$r.squared

ggplot(sim_r_sq_df, aes(x = r_squared)) +
  geom_histogram(bins = 20) +
  geom_vline(aes(xintercept = original_r_squared), color = 'red') +
  labs(title = "Distribution of R-squared Values", caption = "*Red line represents original R-squared value", x = "R-squared", y = "Frequnecy") +
  theme_minimal()
```
